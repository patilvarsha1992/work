import socket
import re
import urllib2
from bs4 import BeautifulSoup
import csv
import threading
import datetime
from datetime import datetime
csvFile = "C:/Users/vpatil/Desktop/google_search_output1_3333.csv"
import MySQLdb
import Queue

queue = Queue.Queue()
class ThreadUrl(threading.Thread):

    def __init__(self,queue ):
        threading.Thread.__init__(self)
        self.name = "UrlGrab"
        self.queue = queue
        return

    def run(self):
        counter = 0

        while True:
            record = self.queue.get()
            jobDone = False
            first_name = record[0]
            print first_name
            last_name = record[1]
            print last_name
            website = record[2]
            print website
            auto_id = record[3]
            li_id = record[4]
            experience_id = record[5]

            fname = first_name.lower()
            lname = last_name.lower()
            domain = website.lower()
            baseurl = 'https://www.google.com/search?q='
            initial_lname = lname[:1].lower()
            initial_fname = fname[:1].lower()

            email_1 = fname + '@' + domain
            email_2 = fname + initial_lname + '@' + domain
            email_3 = initial_fname + lname + '@' + domain
            email_4 = fname + lname + '@' + domain
            email_5 = fname + '.' + initial_lname + '@' + domain
            email_6 = initial_fname + '.' + lname + '@' + domain
            email_7 = fname + '.' + lname + '@' + domain
            email_8 = fname + '_' + initial_lname + '@' + domain
            email_9 = initial_fname + '_' + lname + '@' + domain
            email_10 = fname + '_' + lname + '@' + domain
            email_11 = fname + '-' + initial_lname + '@' + domain
            email_12 = initial_fname + lname + '@' + domain
            email_13 = fname + '-' + lname + '@' + domain
            email_14 = lname + '@' + domain
            email_15 = lname + '.' + fname + '@' + domain
            email_16 = lname + initial_fname + '@' + domain
            email_17 = lname + '_' + fname + '@' + domain
            email_18 = initial_fname + initial_lname + '@' + domain
            email_19 = initial_fname + initial_lname + '2' + '@' + domain
            email_20 = initial_fname + initial_lname + '3' + '@' + domain
            email_21 = initial_fname + initial_lname + '4' + '@' + domain
            email_22 = initial_fname + initial_lname + '5' + '@' + domain
            email_23 = initial_fname + initial_lname + '6' + '@' + domain
            email_24 = initial_fname + initial_lname + '7' + '@' + domain
            email_25 = initial_fname + initial_lname + '8' + '@' + domain

            emails_pattarn = [ email_7, email_2, email_3, email_4, email_5, email_6,email_1, email_8, email_9, email_10,
                              email_11, email_12,
                              email_13, email_14, email_15, email_16, email_17, email_18, email_19, email_20, email_21,
                              email_22, email_23, email_24, email_25]



            for email in emails_pattarn:
                url = baseurl + email + '&hl=en'
                global m
                print '------Auto Id-------'
                print 'Auto_Id=' + str(auto_id)
                print 'Email_pattern-', ':', email
                print '----Original url----'
                print url
                failed = True
                fail_count = 15
                while failed is True:
                    try:
                        proxy = urllib2.ProxyHandler({'https': '209.126.105.212:8081'})
                        opener = urllib2.build_opener(proxy)
                        urllib2.install_opener(opener)
                        opener.addheaders = [('User-agent',
                                              'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36')]
                        socket.setdefaulttimeout(10)
                        open_url = urllib2.urlopen(url)
                        failed = False
                        soup = BeautifulSoup(open_url, 'html.parser')
                        # save data to html file
                        filename = 'html2/test' + str(counter) + '.html'
                        with open(filename, 'w') as f:
                            f.write(str(soup))
                    except Exception as err:
                        print (threading.currentThread().name, " ******** Something went wrong ********: " + str(err))
                        fail_count -= 1
                        print "retries remaining:", fail_count
                        if fail_count <= 0:
                            failed = False
                            print ("no more retry")

                rows = []
                try:
                    for i in xrange(0, 10):
                        cols = [li_id, experience_id, url]
                        blocks = soup.findAll("div", {"class": "g"})

                        print '------URLs-------'
                        get_url_block = soup.findAll("h3", {"class": "r"})
                        get_blocks = get_url_block[i]
                        links = get_blocks.findAll('a')
                        for a in links:
                            get_url = re.split(":(?=http)", a["href"].replace("/url?q=", ""))
                            print get_url
                            cols.append(get_url)

                        try:
                            get_date_block = soup.findAll("span", {"class": "st"})
                            date_block = get_date_block[i]
                            text_date = date_block.get_text()

                            print '----Date Text----'
                            print text_date
                            print '------Date-------'
                            regex = r'[\w]+\s\d{1,2},\s[0-9]{4}'
                            match_date = re.findall(regex, text_date)
                            if len(match_date) > 0:
                                m =match_date[:1]
                                print m
                                cols.append(m)
                            else:
                                msg = ''
                                cols.append(msg)
                                print 'Date Not found'
                        except:
                            print 'DATE LIST INDEX OUT OF RANGE'
                            msg = ''
                            cols.append(msg)

                        if len(blocks) > 0:
                            get_email_block = soup.findAll("span", {"class": "st"})
                            email_blocks = get_email_block[i]
                            text_email = email_blocks.get_text()
                            print '-----Emails------'
                            regex =r'[\w\.-]+@[\w\.-]+'
                            match_email = re.findall(regex, text_email)
                            print 'All found email:',match_email

                            if len(match_email) > 0:
                                for e in match_email:  # First Example
                                    e = e.lower()
                                    print 'email:',e
                                    for i in range(len(emails_pattarn)):
                                        print 'email_pattarn:', emails_pattarn[i]
                                        if e == emails_pattarn[i]:

                                            domain_from_email = e.split("@")[-1]
                                            print 'email_domain:',domain_from_email
                                            print 'url:',get_url
                                            get_url=str(get_url)

                                            if domain_from_email in get_url:
                                                print 'email domain and url domain match '
                                                print 'match email :', e, 'email_pattarn_no:', i + 1
                                                cols.append(e)
                                                cols.append(i+1)
                                                print cols
                                                jobDone = True
                                                break

                                            else:
                                                print 'email domain and url domain not match '
                                        else:
                                            print 'email does not match to pattern'
                            else:
                                print 'email does not match to pattern'
                        else:
                            print 'Not found'
                            msg = ''
                            cols.append(msg)

                        rows.append(cols)
                        print '=========================================XXXXX====================================================================='

                        if jobDone is True:
                            print "email found, no need to go for all blocks"
                            break
                    if jobDone is True:
                        print "email found, no need to go for all patterns"
                        break

                except:
                    print '***********GOOGLE SERACH RESULT NOT FOUND******** '
                    print '=========================================XXXXX====================================================================='

                # store to queue2 instead of wiring to file
            with open(csvFile, "a") as output:
                writer = csv.writer(output, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
                for row in rows:
                    writer.writerow(row)
            self.queue.task_done()
        return
def main():
    start_time1 = datetime.now()
    print  "Start_time" + str(start_time1)
    threadcount = 4
    for i in range(threadcount):
        t = ThreadUrl(queue)
        # t.setDaemon(True)
        t.start()

    db = MySQLdb.connect(
        host="192.168.2.196",
        user="varsha",
        passwd="Varsha123",
        db="adhoc")

    cur = db.cursor()
    query ='select first_name, last_name, domain, id_auto, li_id, experience_id from varsha_01_purple_lizard where id_auto=678'
    cur.execute(query)

    counter=0
    for row in cur.fetchall():
        counter = counter + 1
        first_name = row[0]
        last_name = row[1]
        website = row[2]
        auto_id = row[3]
        li_id = row[4]
        experience_id = row[5]

        queue.put((first_name, last_name, website, auto_id,li_id, experience_id))

    db.close()
    queue.join()
    end_time1 = datetime.now()
    print 'End_Time=' + str(end_time1)
    print('Total_Duration: {}'.format(end_time1 - start_time1))
if __name__ == '__main__':
    main()
